{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data Paths Array: 46\n",
      "Length of Labels Array: 46\n",
      "\n",
      "Fold 1/5\n",
      "Test Healthy Indices: [2 4]\n",
      "Test Unhealthy Indices: [7 8]\n",
      "\n",
      "Fold 2/5\n",
      "Test Healthy Indices: [1 2]\n",
      "Test Unhealthy Indices: [5 6]\n",
      "\n",
      "Fold 3/5\n",
      "Test Healthy Indices: [2 3]\n",
      "Test Unhealthy Indices: [8 5]\n",
      "\n",
      "Fold 4/5\n",
      "Test Healthy Indices: [1 0]\n",
      "Test Unhealthy Indices: [8 4]\n",
      "\n",
      "Fold 5/5\n",
      "Test Healthy Indices: [2 3]\n",
      "Test Unhealthy Indices: [8 5]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Specify the root directory containing the subfolders\n",
    "root_directory = '/Users/GraceWang/Desktop/UNLV2023/hearingLossOriginalCodebase/REU-Hearing-Loss-Project/machine learning/data split by subject'\n",
    "\n",
    "# Create a list of paths to the subfolders (classes)\n",
    "subfolder_paths = [os.path.join(root_directory, \"Healthy\"), os.path.join(root_directory, \"Hearing Impaired\")]\n",
    "\n",
    "# Combine the data paths and corresponding labels (0 for Healthy, 1 for Hearing Impaired)\n",
    "data_paths = []\n",
    "labels = []\n",
    "\n",
    "# Enumerate over subfolder_paths (0 for Healthy, 1 for Hearing Impaired)\n",
    "for label, subfolder_path in enumerate(subfolder_paths):\n",
    "    # Iterate over subject folders within each class\n",
    "    for subject_folder in os.listdir(subfolder_path):\n",
    "        # Construct the full path to the subject folder\n",
    "        subject_path = os.path.join(subfolder_path, subject_folder)\n",
    "        # Append the subject path to data_paths\n",
    "        data_paths.append(subject_path)\n",
    "        # Append the corresponding label to labels\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert lists to arrays\n",
    "data_paths = np.array(data_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "# Print the length of the arrays\n",
    "print(\"Length of Data Paths Array:\", len(data_paths))\n",
    "print(\"Length of Labels Array:\", len(labels))\n",
    "# Number of folds for k-fold cross-validation\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# CNN Model\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Iterate over K folds\n",
    "for fold, (train_indices, test_indices) in enumerate(skf.split(data_paths, labels)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Extract paths for training and testing\n",
    "    train_paths, test_paths = data_paths[train_indices], data_paths[test_indices]\n",
    "\n",
    "    # Define the number of healthy and unhealthy subjects for test\n",
    "    num_test_healthy = 2\n",
    "    num_test_unhealthy = 2\n",
    "\n",
    "    # Lists to store results for each fold\n",
    "    fold_accuracies = []\n",
    "\n",
    "    # Hold out different healthy and unhealthy subjects for each training and average the results\n",
    "    # Print indices for healthy subjects in the test set\n",
    "    test_healthy_indices = np.random.choice(np.where(labels[test_indices] == 0)[0], num_test_healthy, replace=False)\n",
    "    print(\"Test Healthy Indices:\", test_healthy_indices)\n",
    "\n",
    "    # Print indices for unhealthy subjects in the test set\n",
    "    test_unhealthy_indices = np.random.choice(np.where(labels[test_indices] == 1)[0], num_test_unhealthy, replace=False)\n",
    "    print(\"Test Unhealthy Indices:\", test_unhealthy_indices)\n",
    "\n",
    "\n",
    "# Note: You may need to adjust the paths and other parameters based on your specific dataset structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can replicate K fold... I have 2 folders, healthy and hearing impaired (create a copy of both).\n",
    "#  I will take 2 subjects from each folder by random and add all their images to a testing folder with labels\n",
    "# I will take the rest of the subjects, label each of their images based on whcih folder they're in, and then merge \n",
    "# all the images together to create the training dataset. And I will do this 5 different times, to replicate K fold, \n",
    "# and average the accuracies\n",
    "\n",
    "\n",
    "# To summarize your approach:\n",
    "# Replication of K Folds:\n",
    "# You have two folders, one for \"healthy\" and one for \"hearing impaired.\"\n",
    "# You create copies of both folders.\n",
    "# Test Set Creation:\n",
    "# For each iteration (replicating a fold):\n",
    "# Randomly select two subjects from each folder.\n",
    "# Create a testing folder with images from these selected subjects, labeled accordingly.\n",
    "# Training Set Creation:\n",
    "# For each iteration:\n",
    "# Take the remaining subjects (excluding the ones selected for testing).\n",
    "# Label each image based on its folder (\"healthy\" or \"hearing impaired\").\n",
    "# Merge all the labeled images to create the training dataset.\n",
    "# Replication and Averaging:\n",
    "# Repeat the above process for a total of 5 iterations to replicate k-fold cross-validation.\n",
    "# Average the accuracies obtained from each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data Paths Array: 46\n",
      "Length of Labels Array: 46\n",
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "listdir: embedded null character in path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m     train_paths_for_iteration \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(train_paths_for_iteration, np\u001b[38;5;241m.\u001b[39mwhere(train_paths_for_iteration \u001b[38;5;241m==\u001b[39m test_subject_path))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Create data generator for the current iteration\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_paths_for_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Create data generator for the current iteration\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Clone the model to ensure a new instance is used for each training iteration\u001b[39;00m\n\u001b[1;32m     97\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/preprocessing/image.py:1649\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow_from_directory\u001b[39m(\n\u001b[1;32m   1563\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1564\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     keep_aspect_ratio\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1579\u001b[0m ):\n\u001b[1;32m   1580\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \n\u001b[1;32m   1582\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[39m        and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1649\u001b[0m     \u001b[39mreturn\u001b[39;00m DirectoryIterator(\n\u001b[1;32m   1650\u001b[0m         directory,\n\u001b[1;32m   1651\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1652\u001b[0m         target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[1;32m   1653\u001b[0m         color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[1;32m   1654\u001b[0m         keep_aspect_ratio\u001b[39m=\u001b[39;49mkeep_aspect_ratio,\n\u001b[1;32m   1655\u001b[0m         classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[1;32m   1656\u001b[0m         class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[1;32m   1657\u001b[0m         data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[1;32m   1658\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1659\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1660\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   1661\u001b[0m         save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[1;32m   1662\u001b[0m         save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[1;32m   1663\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[1;32m   1664\u001b[0m         follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[1;32m   1665\u001b[0m         subset\u001b[39m=\u001b[39;49msubset,\n\u001b[1;32m   1666\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[1;32m   1667\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m   1668\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/preprocessing/image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[1;32m    562\u001b[0m     classes \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[1;32m    564\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    565\u001b[0m             classes\u001b[39m.\u001b[39mappend(subdir)\n",
      "\u001b[0;31mValueError\u001b[0m: listdir: embedded null character in path"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Specify the root directory containing the subfolders\n",
    "root_directory = '/Users/GraceWang/Desktop/UNLV2023/hearingLossOriginalCodebase/REU-Hearing-Loss-Project/machine learning/data split by subject'\n",
    "\n",
    "# Create a list of paths to the subfolders (classes)\n",
    "subfolder_paths = [os.path.join(root_directory, \"Healthy\"), os.path.join(root_directory, \"Hearing Impaired\")]\n",
    "\n",
    "# Combine the data paths and corresponding labels (0 for Healthy, 1 for Hearing Impaired)\n",
    "data_paths = []\n",
    "labels = []\n",
    "\n",
    "# Enumerate over subfolder_paths (0 for Healthy, 1 for Hearing Impaired)\n",
    "for label, subfolder_path in enumerate(subfolder_paths):\n",
    "    # Iterate over subject folders within each class\n",
    "    for subject_folder in os.listdir(subfolder_path):\n",
    "        # Construct the full path to the subject folder\n",
    "        subject_path = os.path.join(subfolder_path, subject_folder)\n",
    "        # Append the subject path to data_paths\n",
    "        data_paths.append(subject_path)\n",
    "        # Append the corresponding label to labels\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert lists to arrays\n",
    "data_paths = np.array(data_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "# Print the length of the arrays\n",
    "print(\"Length of Data Paths Array:\", len(data_paths))\n",
    "print(\"Length of Labels Array:\", len(labels))\n",
    "# Number of folds for k-fold cross-validation\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# CNN Model\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Iterate over K folds\n",
    "for fold, (train_indices, test_indices) in enumerate(skf.split(data_paths, labels)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Extract paths for training and testing\n",
    "    train_paths, test_paths = data_paths[train_indices], data_paths[test_indices]\n",
    "\n",
    "    # Define the number of healthy and unhealthy subjects for test\n",
    "    num_test_healthy = 2\n",
    "    num_test_unhealthy = 2\n",
    "\n",
    "    # Lists to store results for each fold\n",
    "    fold_accuracies = []\n",
    "\n",
    "    # Hold out different healthy and unhealthy subjects for each training and average the results\n",
    "    test_healthy_indices = np.random.choice(np.where(labels[test_indices] == 0)[0], num_test_healthy, replace=False)\n",
    "    test_unhealthy_indices = np.random.choice(np.where(labels[test_indices] == 1)[0], num_test_unhealthy, replace=False)\n",
    "    \n",
    "    test_subjects_indices = np.concatenate([test_healthy_indices, test_unhealthy_indices])\n",
    "    test_subjects_paths = test_paths[test_subjects_indices]\n",
    "    try_count = 0\n",
    "\n",
    "    for test_subject_path in test_subjects_paths:\n",
    "        # Exclude the current test subject\n",
    "        train_paths_for_iteration = train_paths.copy()\n",
    "        # train_paths_for_iteration = np.append(train_paths_for_iteration, test_subject_path)\n",
    "        if test_subject_path in train_paths_for_iteration:\n",
    "            train_paths_for_iteration = np.delete(train_paths_for_iteration, np.where(train_paths_for_iteration == test_subject_path))\n",
    "        # Create data generator for the current iteration\n",
    "        train_generator = ImageDataGenerator(rescale=1.0 / 255.0).flow_from_directory(\n",
    "            train_paths_for_iteration,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        # Create data generator for the current iteration\n",
    "   \n",
    "        # Clone the model to ensure a new instance is used for each training iteration\n",
    "        model = build_model((224, 224, 3))\n",
    "\n",
    "        # Train the model for the current iteration\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "            epochs=60)\n",
    "\n",
    "        # Evaluate the model on the test subjects for the current iteration\n",
    "        test_generator = ImageDataGenerator(rescale=1.0 / 255.0).flow_from_directory(\n",
    "            test_subject_path,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "        print(\"Test Loss:\", test_loss)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "        fold_accuracies.append(test_accuracy)\n",
    "\n",
    "    # Average the accuracy results for the test subjects\n",
    "    fold_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Average Accuracy for Fold {fold + 1}: {fold_accuracy}\")\n",
    "\n",
    "# Note: You may need to adjust the paths and other parameters based on your specific dataset structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '/Users/GraceWang/Desktop/UNLV2023/hearingLossOriginalCodebase/REU-Hearing-Loss-Project/machine learning/data split by subject/Healthy/Subject_037'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(subject_part\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Sort the list based on the subject number extracted from the image names\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m sorted_data_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_tuples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_subject_number_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Print the sorted data paths and corresponding labels\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_path, label \u001b[38;5;129;01min\u001b[39;00m sorted_data_tuples:\n",
      "Cell \u001b[0;32mIn[24], line 61\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(subject_part\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Sort the list based on the subject number extracted from the image names\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m sorted_data_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(data_tuples, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mextract_subject_number_from_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Print the sorted data paths and corresponding labels\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_path, label \u001b[38;5;129;01min\u001b[39;00m sorted_data_tuples:\n",
      "Cell \u001b[0;32mIn[24], line 58\u001b[0m, in \u001b[0;36mextract_subject_number_from_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m subject_part \u001b[38;5;241m=\u001b[39m image_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_sub-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Extract the numeric part and convert it to an integer\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubject_part\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '/Users/GraceWang/Desktop/UNLV2023/hearingLossOriginalCodebase/REU-Hearing-Loss-Project/machine learning/data split by subject/Healthy/Subject_037'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#  data is organized as follows:\n",
    "# - Data split by subject folder\n",
    "#   - Healthy folder\n",
    "#     - Subject_001 folder\n",
    "        # - 40 png images\n",
    "#     - Subject_002 folder\n",
    "        # - 40 png images\n",
    "#     ...\n",
    "#   - Hearing Impaired folder\n",
    "#     - Subject_003 folder\n",
    "        # - 40 png images\n",
    "#     - Subject_004 folder\n",
    "        # - 40 png images\n",
    "#     ...\n",
    "\n",
    "# Specify the root directory containing the subfolders\n",
    "root_directory = '/Users/GraceWang/Desktop/UNLV2023/hearingLossOriginalCodebase/REU-Hearing-Loss-Project/machine learning/data split by subject'\n",
    "\n",
    "# Create a list of paths to the subfolders (classes)\n",
    "subfolder_paths = [os.path.join(root_directory, \"Healthy\"), os.path.join(root_directory, \"Hearing Impaired\")]\n",
    "\n",
    "# Combine the data paths and corresponding labels (0 for Healthy, 1 for Hearing Impaired)\n",
    "data_paths = []\n",
    "labels = []\n",
    "\n",
    "# Enumerate over subfolder_paths (0 for Healthy, 1 for Hearing Impaired)\n",
    "for label, subfolder_path in enumerate(subfolder_paths):\n",
    "    # Iterate over subject folders within each class\n",
    "    for subject_folder in os.listdir(subfolder_path):\n",
    "        # Construct the full path to the subject folder\n",
    "        subject_path = os.path.join(subfolder_path, subject_folder)\n",
    "        # Append the subject path to data_paths\n",
    "        data_paths.append(subject_path)        \n",
    "        # Append the corresponding label to labels\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert lists to arrays\n",
    "data_paths = np.array(data_paths)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Number of folds for k-fold cross-validation\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# CNN Model\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# Iterate over K folds\n",
    "for fold, (train_indices, test_indices) in enumerate(skf.split(data_paths, labels)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Extract paths for training and testing\n",
    "    train_paths, test_paths = data_paths[train_indices], data_paths[test_indices]\n",
    "\n",
    "    # Split the training set into training and validation sets\n",
    "    # train_paths, val_paths = train_test_split(train_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Dimensions of the images (224x224 with 3 channels)\n",
    "    img_width, img_height = 224, 224\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "    # Create the CNN model\n",
    "    model = build_model(input_shape)\n",
    "    model.summary()\n",
    "\n",
    "    # Data generators for training, validation, and testing\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "    val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_paths,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        test_paths,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # Training the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "        epochs=60,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=val_generator.samples // val_generator.batch_size)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_accuracy = model.evaluate(val_generator, steps=val_generator.samples // val_generator.batch_size)\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    y_true = val_generator.classes\n",
    "    y_pred = np.argmax(model.predict(val_generator), axis=-1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(val_generator.class_indices))\n",
    "    plt.xticks(tick_marks, val_generator.class_indices.keys(), rotation=45)\n",
    "    plt.yticks(tick_marks, val_generator.class_indices.keys())\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    print(cm)\n",
    "\n",
    "    # Classification Report\n",
    "    class_names = list(val_generator.class_indices.keys())\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement the strategy of holding out different test subjects for each training and averaging the results, \n",
    "# you can modify the code within your k-fold cross-validation loop. Here's a modified version of your code to incorporate \n",
    "# this approach:\n",
    "\n",
    "\n",
    "# ... (Your previous code remains unchanged up to this point)\n",
    "\n",
    "# Number of folds for k-fold cross-validation\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the number of test subjects to hold out for evaluation\n",
    "num_test_subjects = 3\n",
    "\n",
    "# Lists to store results for each fold\n",
    "fold_accuracies = []\n",
    "\n",
    "# Iterate over K folds\n",
    "for fold, (train_indices, test_indices) in enumerate(skf.split(data_paths, labels)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Extract paths for training and testing\n",
    "    train_paths, test_paths = data_paths[train_indices], data_paths[test_indices]\n",
    "\n",
    "    # Hold out different test subjects for each training and average the results\n",
    "    test_subjects_indices = np.random.choice(len(test_paths), num_test_subjects, replace=False)\n",
    "    test_subjects_paths = test_paths[test_subjects_indices]\n",
    "\n",
    "    fold_accuracies_for_test_subjects = []\n",
    "\n",
    "    for test_subject_path in test_subjects_paths:\n",
    "        # Exclude the current test subject\n",
    "        train_paths_for_iteration = train_paths.copy()\n",
    "        train_paths_for_iteration = np.append(train_paths_for_iteration, test_subject_path)\n",
    "        \n",
    "        # Create data generators for the current iteration\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_paths_for_iteration,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        # Clone the model to ensure a new instance is used for each training iteration\n",
    "        model = build_model(input_shape)\n",
    "\n",
    "        # Train the model for the current iteration\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "            epochs=60)\n",
    "\n",
    "        # Evaluate the model on the test subjects for the current iteration\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            test_subject_path,\n",
    "            target_size=(img_width, img_height),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        test_loss, test_accuracy = model.evaluate(val_generator, steps=val_generator.samples // val_generator.batch_size)\n",
    "        print(\"Test Loss:\", test_loss)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "        fold_accuracies_for_test_subjects.append(test_accuracy)\n",
    "\n",
    "    # Average the accuracy results for the test subjects\n",
    "    fold_accuracy = np.mean(fold_accuracies_for_test_subjects)\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"\\nAverage Accuracy Across All Folds: {average_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
